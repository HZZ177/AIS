from crewai import Task


class Tasks:
    @staticmethod
    def source_code_analysis_task(agent) -> Task:
        source_code_analysis_task = Task(
            description="""
以下是提供给你的源码信息：
{source_data}\n

你的任务是阅读分析提供给你的这些源码信息，并分析出针对其中潜在问题测试点。这是一些互有关联的，能够组成一个完整功能的函数调用链中各个函数的源码
其中entry_point对应的值是入口函数的函数名，其他源码是实现功能的调用链上的各个函数源码
1、根据提供给你的这些源代码调用链，请先分析这部分源码的逻辑，然后描述你分析出的功能
2、然后需要你分析潜在的问题，包括但不限于诸如，功能性，边界性，异常场景处理遗漏，性能隐患，逻辑缺失等问题
你需要针对其中所有的潜在问题生成对应测试点，传递给下一个测试人员，方便他更加详细的生成测试用例
你只能使用中文来沟通和回答，如果没有接收到源码，直接返回没有接收到任何源码，无法分析

这还有一些额外辅助信息：
{extra_info}\n

返回测试点请严格按照以下格式：
    【代码功能描述】：xxxxx

    代码中潜在的问题1：
        【代码片段】：xxxxx
        【潜在的问题】xxxxx
        【测试点】：xxxx
    代码中潜在的问题2：
        【代码片段】：xxxxx
        【潜在的问题】xxxxx
        【测试点】：xxxx
    代码中潜在的问题3：
        【代码片段】：xxxxx
        【潜在的问题】xxxxx
        【测试点】：xxxx
    ......
    
    除此之外不要有任何额外信息，也不要有测试用例，只需要输出测试点
            """,
            expected_output="""
1、分析的代码功能描述
2、分点列出的根据源码内容分析出的测试点
            """,
            agent=agent,
            verbose=True,
        )
        return source_code_analysis_task

    @staticmethod
    def test_point_checker_task(agent):
        test_point_checker_task = Task(
            description="""
【源码信息】
{source_data}
【额外辅助信息】
{extra_info}

根据提供给你的源码，审核前一个测试员拆分的测试点
如果没有接收到任何测试点信息，请直接返回未接收到测试点信息，无法继续并终结任务
如果接收到有测试点信息，请继续：
1、如果你认为测试点已经完全覆盖需求，直接将测试点一模一样返回，格式如下：
    评审结果：【评审通过】不需要修改，以下是测试点列表：
    （此处为上一个测试人员拆分出来的所有测试点）
    ......

2、如果你认为有不足，在已有测试点的基础上尽量不删除并进行优化补充，然后分成
1、上一个测试人员分析的通过的测试点部分（这部分请你不做任何修改的复制传给你的上一个测试人员拆分的测试点）
2、经过评审新增加/修改的测试点部分（这部分请你输出你认为在原测试点分析上有修改或者有遗漏的地方）
两部分返回你修改后的最终结果

请严格按照以下格式返回测试点：
    评审结果—【评审不通过】
    以下是修改优化后的测试点：
    
    【上一个测试人员分析的通过的测试点部分】：
        代码中潜在的问题1：
            【代码片段】：xxxxx
            【潜在的问题】xxxxx
            【测试点】：xxxx
        代码中潜在的问题2：
            【代码片段】：xxxxx
            【潜在的问题】xxxxx
            【测试点】：xxxx
        代码中潜在的问题3：
            【代码片段】：xxxxx
            【潜在的问题】xxxxx
            【测试点】：xxxx
    【经过评审新增加/修改的测试点部分】：
        代码中潜在的问题1：
            【代码片段】：xxxxx
            【潜在的问题】xxxxx
            【测试点】：xxxx
        代码中潜在的问题2：
            【代码片段】：xxxxx
            【潜在的问题】xxxxx
            【测试点】：xxxx
        代码中潜在的问题3：
            【代码片段】：xxxxx
            【潜在的问题】xxxxx
            【测试点】：xxxx
    ......

除此之外不要有任何额外信息
将你最终的结果完整的传递给下一个测试用例生成人员
            """,
            expected_output="""基于需求以及前一位测试人员拆分的测试点的基础上，评审优化后的测试点""",
            agent=agent,
            verbose=True,
            # human_input=True
        )
        return test_point_checker_task

    @staticmethod
    def testcase_generate_task(agent):
        testcase_task = Task(
            description=r"""
1、根据提供给你的信息编写测试用例，每一个测试点应该对应多条用例，使用等价类，边界值，异常值等方法，尽量全面的设计用例
2、如果没有接收到任何测试点，请直接返回没有接收到任何测试点信息，然后完结任务
3、如果有测试点信息，返回用例的格式用markdown语法，且除了标题分级外不要用其他的语法例如加粗，斜体等，下面是一个用例的格式示例:
# 总名称
## 对应风险点：（此处为传给你的 代码中潜在的问题x 的原文，如：代码中潜在的问题2：【代码片段】：xxxxx【潜在的问题】xxxxx【测试点】：xxxx）
### 用例标题（例如：用例一：xxxx）
#### 【前置条件】
#### 步骤
    ##### 步骤1
    ##### 步骤2
#### 预期结果
    ##### 预期结果2
    ##### 预期结果3
4、不同参数类型的用例用分成不同的用例，而不是在对应这个测试点的步骤中分成多个步骤，
比如验证某个配置的测试点，不填该配置，填入生效的值，填入不生效的值，应该分为三个用例，而不是一个用例的三个步骤对应三种预期结果
5、标题分级的要求是，无论怎么分类，最好第三级是用例标题，最多到第四级是用例标题；不要使用代码块，代码块会导致导入xmind时内容进入备注里
            """,
            expected_output="测试用例列表",
            agent=agent,
            verbose=True
        )
        return testcase_task
